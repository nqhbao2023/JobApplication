# PLAN: H·ªÜ TH·ªêNG THU TH·∫¨P D·ªÆ LI·ªÜU C√îNG VI·ªÜC CHO JOB_4S (ƒê·ªí √ÅN SINH VI√äN)

## üéØ M·ª§C TI√äU TH·ª∞C T·∫æ

### ‚ö†Ô∏è V·∫•n ƒë·ªÅ ban ƒë·∫ßu:
1. **TopCV, VietnamWorks robots.txt ch·∫∑n crawl** ‚Üí Kh√¥ng ƒë∆∞·ª£c ph√©p crawl tr·ª±c ti·∫øp
2. **Kh√¥ng c√≥ m√°y ch·∫°y 24/7** ‚Üí C·∫ßn gi·∫£i ph√°p cloud mi·ªÖn ph√≠
3. **Employer gi·∫£ (do admin t·∫°o)** ‚Üí C·∫ßn x·ª≠ l√Ω khi candidate nh·∫•n apply/view job
4. **Ph√°p l√Ω**: Crawl c√≥ th·ªÉ vi ph·∫°m ToS, copyright JD/logo

### ‚úÖ GI·∫¢I PH√ÅP T√åM ƒê∆Ø·ª¢C:
**Crawl t·ª´ viecoi.vn** - Trang tuy·ªÉn d·ª•ng Vi·ªát Nam cho ph√©p crawl h·ª£p ph√°p!

**Ki·ªÉm tra robots.txt**: https://viecoi.vn/robots.txt
- ‚úÖ `Allow: /viec-lam/*.html$` ‚Üí ƒê∆∞·ª£c ph√©p crawl trang chi ti·∫øt job (c√≥ JD ƒë·∫ßy ƒë·ªß)
- ‚úÖ `Allow: /tim-cong-ty/*.html$` v√† `/gioi-thieu-cong-ty/*.html$` ‚Üí ƒê∆∞·ª£c ph√©p crawl c√¥ng ty
- ‚úÖ `Allow: /*.xml$` ‚Üí ƒê∆∞·ª£c ph√©p crawl sitemap (https://viecoi.vn/sitemap.xml)
- ‚úÖ Ngu·ªìn Vi·ªát Nam, d·ªØ li·ªáu ph√π h·ª£p v·ªõi sinh vi√™n t·∫°i B√¨nh D∆∞∆°ng v√† c√°c t·ªânh l√¢n c·∫≠n

---

## üí° GI·∫¢I PH√ÅP CH√çNH TH·ª®C: CRAWL T·ª™ VIECOI.VN

### **T·∫°i sao ch·ªçn viecoi.vn?**
1. **H·ª£p ph√°p 100%**: robots.txt cho ph√©p crawl job, c√¥ng ty, sitemap
2. **D·ªØ li·ªáu Vi·ªát Nam**: Ph√π h·ª£p v·ªõi ƒë·ªëi t∆∞·ª£ng sinh vi√™n Vi·ªát Nam
3. **JD ƒë·∫ßy ƒë·ªß**: C√≥ th·ªÉ l·∫•y m√¥ t·∫£ c√¥ng vi·ªác, y√™u c·∫ßu, l∆∞∆°ng, k·ªπ nƒÉng, c√¥ng ty...
4. **D·ªÖ crawl**: HTML structure r√µ r√†ng, kh√¥ng c√≥ Cloudflare/captcha ph·ª©c t·∫°p
5. **ƒê·ªß s·ªë l∆∞·ª£ng**: H√†ng ngh√¨n job ƒëang ho·∫°t ƒë·ªông

### **Quy tr√¨nh crawl:**
```
1. Crawl sitemap.xml ‚Üí L·∫•y danh s√°ch URL job + c√¥ng ty
2. Parse t·ª´ng URL job ‚Üí L·∫•y JD, l∆∞∆°ng, k·ªπ nƒÉng, ƒë·ªãa ƒëi·ªÉm, lo·∫°i h√¨nh, c√¥ng ty...
3. Parse t·ª´ng URL c√¥ng ty ‚Üí L·∫•y t√™n, m√¥ t·∫£, ƒë·ªãa ch·ªâ, ng√†nh ngh·ªÅ, logo...
4. Normalize data ‚Üí Map v·ªÅ taxonomy c·ªßa Job_4S
5. Deduplicate ‚Üí Lo·∫°i b·ªè job tr√πng l·∫∑p
6. Upsert v√†o Firestore ‚Üí Batch insert/update
7. Sync l√™n Algolia ‚Üí T√¨m ki·∫øm nhanh
```

---

## üèóÔ∏è KI·∫æN TR√öC H·ªÜ TH·ªêNG

### 1. **GitHub Actions Workflow** (thay v√¨ cron 24/7)
```yaml
# .github/workflows/sync-jobs-viecoi.yml
name: Sync Jobs from ViecoiVN

on:
  schedule:
    - cron: '0 2 * * *'  # Ch·∫°y m·ªói ng√†y 02:00 (gi·∫£m t·∫£i server)
  workflow_dispatch:      # Ho·∫∑c ch·∫°y th·ªß c√¥ng

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
      - run: npm install
      - run: npm run crawl:viecoi-sitemap
        env:
          FIREBASE_SERVICE_ACCOUNT: ${{ secrets.FIREBASE_SA }}
          ALGOLIA_APP_ID: ${{ secrets.ALGOLIA_APP_ID }}
          ALGOLIA_API_KEY: ${{ secrets.ALGOLIA_API_KEY }}
      - run: npm run crawl:viecoi-jobs
      - run: npm run crawl:viecoi-companies
      - run: npm run sync:to-algolia
```

**L·ª£i √≠ch:**
- ‚úÖ Mi·ªÖn ph√≠ (2000 ph√∫t/th√°ng)
- ‚úÖ Kh√¥ng c·∫ßn m√°y c√° nh√¢n b·∫≠t
- ‚úÖ Logs chi ti·∫øt, d·ªÖ debug
- ‚úÖ Secrets management an to√†n
- ‚úÖ Crawl t·ª± ƒë·ªông h√†ng ng√†y

---

### 2. **C·∫•u tr√∫c code chi ti·∫øt**
```
/server/src/
‚îú‚îÄ‚îÄ crawlers/
‚îÇ   ‚îú‚îÄ‚îÄ viecoi/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts                    # Entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SitemapCrawler.ts           # Crawl sitemap.xml ‚Üí l·∫•y URLs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ JobCrawler.ts               # Crawl /viec-lam/*.html ‚Üí JD ƒë·∫ßy ƒë·ªß
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CompanyCrawler.ts           # Crawl /tim-cong-ty/*.html ‚Üí C√¥ng ty
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ JobParser.ts            # Parse HTML job ‚Üí Extract data
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CompanyParser.ts        # Parse HTML company ‚Üí Extract data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ httpClient.ts           # Axios + retry logic
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ rateLimiter.ts          # Throttle requests
‚îÇ   ‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Normalizer.ts               # Map v·ªÅ taxonomy Job_4S
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Deduplicator.ts             # Fuzzy match job titles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Validator.ts                # Validate data tr∆∞·ªõc khi upsert
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Enricher.ts                 # Th√™m metadata (slug, timestamps...)
‚îÇ   ‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FirestoreUpserter.ts        # Batch upsert jobs/companies
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AlgoliaSync.ts              # Incremental sync to Algolia
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ logger.ts                   # Winston logger
‚îÇ       ‚îî‚îÄ‚îÄ mappings.ts                 # City, industry, job type mappings

/server/data/
‚îú‚îÄ‚îÄ viecoi-sitemap-cache.json           # Cache sitemap ƒë·ªÉ tr√°nh re-crawl
‚îú‚îÄ‚îÄ viecoi-jobs-raw.json                # Raw data t·ª´ viecoi
‚îî‚îÄ‚îÄ viecoi-companies-raw.json           # Raw companies data

/server/scripts/
‚îú‚îÄ‚îÄ crawl-viecoi-sitemap.ts             # Script crawl sitemap
‚îú‚îÄ‚îÄ crawl-viecoi-jobs.ts                # Script crawl jobs
‚îú‚îÄ‚îÄ crawl-viecoi-companies.ts           # Script crawl companies
‚îî‚îÄ‚îÄ sync-to-algolia.ts                  # Script sync l√™n Algolia

/.github/workflows/
‚îî‚îÄ‚îÄ sync-jobs-viecoi.yml                # GitHub Actions workflow
```

---

## üìã QUY TR√åNH CRAWL CHI TI·∫æT

### **B∆∞·ªõc 1: Crawl Sitemap**
```typescript
// crawl-viecoi-sitemap.ts
import axios from 'axios';
import xml2js from 'xml2js';

async function crawlSitemap() {
  const sitemapUrl = 'https://viecoi.vn/sitemap.xml';
  const res = await axios.get(sitemapUrl);
  const parsed = await xml2js.parseStringPromise(res.data);
  
  // L·ªçc URL jobs v√† companies
  const jobUrls = [];
  const companyUrls = [];
  
  for (const url of parsed.urlset.url) {
    const loc = url.loc[0];
    if (loc.includes('/viec-lam/') && loc.endsWith('.html')) {
      jobUrls.push(loc);
    } else if (loc.includes('/tim-cong-ty/') || loc.includes('/gioi-thieu-cong-ty/')) {
      companyUrls.push(loc);
    }
  }
  
  console.log(`Found ${jobUrls.length} jobs, ${companyUrls.length} companies`);
  
  // L∆∞u cache ƒë·ªÉ tr√°nh re-crawl
  await saveToFile('viecoi-sitemap-cache.json', { jobUrls, companyUrls, lastUpdated: new Date() });
  
  return { jobUrls, companyUrls };
}
```

**K·∫øt qu·∫£**: Danh s√°ch URL c·ªßa t·∫•t c·∫£ jobs v√† companies

---

### **B∆∞·ªõc 2: Crawl Job Details**
```typescript
// crawl-viecoi-jobs.ts
import axios from 'axios';
import * as cheerio from 'cheerio';
import { RateLimiter } from './utils/rateLimiter';

const rateLimiter = new RateLimiter({ maxRequests: 10, perSeconds: 1 }); // 10 req/gi√¢y

async function crawlJob(url: string) {
  await rateLimiter.wait();
  
  const res = await axios.get(url, {
    headers: { 'User-Agent': 'Job4S-Crawler/1.0 (Educational Project)' }
  });
  
  const $ = cheerio.load(res.data);
  
  // Parse HTML ‚Üí Extract data
  const job = {
    title: $('.job-title').text().trim(),
    company_name: $('.company-name').text().trim(),
    location: $('.job-location').text().trim(),
    salary: $('.job-salary').text().trim(),
    job_type: $('.job-type').text().trim(), // Full-time, Part-time...
    deadline: $('.job-deadline').text().trim(),
    description: $('.job-description').html(),
    requirements: $('.job-requirements').html(),
    benefits: $('.job-benefits').html(),
    skills: $('.job-skills .skill-tag').map((i, el) => $(el).text().trim()).get(),
    experience: $('.job-experience').text().trim(),
    education: $('.job-education').text().trim(),
    quantity: $('.job-quantity').text().trim(),
    gender: $('.job-gender').text().trim(),
    
    // Metadata
    source: 'viecoi',
    external_url: url,
    application_type: 'external', // Redirect v·ªÅ viecoi khi apply
    crawled_at: new Date(),
  };
  
  return job;
}

async function crawlAllJobs(jobUrls: string[]) {
  const jobs = [];
  
  for (const url of jobUrls.slice(0, 100)) { // Test v·ªõi 100 jobs ƒë·∫ßu ti√™n
    try {
      const job = await crawlJob(url);
      jobs.push(job);
      console.log(`Crawled: ${job.title} at ${job.company_name}`);
    } catch (error) {
      console.error(`Failed to crawl ${url}:`, error.message);
    }
  }
  
  await saveToFile('viecoi-jobs-raw.json', jobs);
  return jobs;
}
```

**L∆∞u √Ω HTML selectors**: 
- C·∫ßn inspect trang viecoi.vn ƒë·ªÉ l·∫•y ƒë√∫ng class names
- C√≥ th·ªÉ thay ƒë·ªïi theo th·ªùi gian, c·∫ßn maintain

---

### **B∆∞·ªõc 3: Crawl Company Details**
```typescript
// crawl-viecoi-companies.ts
async function crawlCompany(url: string) {
  await rateLimiter.wait();
  
  const res = await axios.get(url, {
    headers: { 'User-Agent': 'Job4S-Crawler/1.0 (Educational Project)' }
  });
  
  const $ = cheerio.load(res.data);
  
  const company = {
    name: $('.company-name').text().trim(),
    logo: $('.company-logo img').attr('src'),
    industry: $('.company-industry').text().trim(),
    address: $('.company-address').text().trim(),
    website: $('.company-website').attr('href'),
    size: $('.company-size').text().trim(), // 50-100, 100-500...
    description: $('.company-description').html(),
    
    // Metadata
    source: 'viecoi',
    external_url: url,
    crawled_at: new Date(),
  };
  
  return company;
}
```

---

### **B∆∞·ªõc 4: Normalize Data**
```typescript
// processors/Normalizer.ts
function normalizeJob(rawJob: any) {
  return {
    id: generateSlug(rawJob.title, rawJob.company_name),
    title: rawJob.title,
    company_id: null, // S·∫Ω map sau khi c√≥ companies
    company_name: rawJob.company_name,
    
    // Map location v·ªÅ taxonomy
    location: mapLocation(rawJob.location), // H√† N·ªôi, TP.HCM, B√¨nh D∆∞∆°ng...
    
    // Map job type
    job_type_id: mapJobType(rawJob.job_type), // full-time, part-time, intern...
    
    // Map category
    category: extractCategory(rawJob.title, rawJob.description),
    
    // Parse salary
    salary_min: parseSalary(rawJob.salary).min,
    salary_max: parseSalary(rawJob.salary).max,
    currency: 'VND',
    
    // Extract skills
    skills: extractSkills(rawJob.skills, rawJob.requirements),
    
    description: sanitizeHTML(rawJob.description),
    requirements: parseRequirements(rawJob.requirements),
    benefits: parseBenefits(rawJob.benefits),
    
    // Metadata
    source: rawJob.source,
    application_type: rawJob.application_type,
    external_url: rawJob.external_url,
    is_verified: false, // Admin ch∆∞a ki·ªÉm duy·ªát
    is_crawled: true,
    
    // Timestamps
    created_at: admin.firestore.FieldValue.serverTimestamp(),
    updated_at: admin.firestore.FieldValue.serverTimestamp(),
    expires_at: parseDeadline(rawJob.deadline),
    
    status: 'draft', // Admin s·∫Ω approve th√†nh 'active'
  };
}
```

---

### **B∆∞·ªõc 5: Deduplicate**
```typescript
// processors/Deduplicator.ts
import Fuse from 'fuse.js';

async function deduplicateJobs(jobs: any[]) {
  const existingJobs = await getExistingJobsFromFirestore();
  
  const fuse = new Fuse(existingJobs, {
    keys: ['title', 'company_name', 'location'],
    threshold: 0.3,
  });
  
  const uniqueJobs = [];
  
  for (const job of jobs) {
    const matches = fuse.search(`${job.title} ${job.company_name}`);
    
    if (matches.length === 0) {
      uniqueJobs.push(job);
    } else {
      console.log(`Duplicate found: ${job.title} at ${job.company_name}`);
      // Update existing job thay v√¨ t·∫°o m·ªõi
      await updateJob(matches[0].item.id, job);
    }
  }
  
  return uniqueJobs;
}
```

---

### **B∆∞·ªõc 6: Upsert v√†o Firestore**
```typescript
// storage/FirestoreUpserter.ts
async function upsertJobs(jobs: any[]) {
  const batch = admin.firestore().batch();
  const jobsRef = admin.firestore().collection('jobs');
  
  for (const job of jobs) {
    const docRef = jobsRef.doc(job.id);
    batch.set(docRef, job, { merge: true });
  }
  
  await batch.commit();
  console.log(`Upserted ${jobs.length} jobs to Firestore`);
}
```

---

### **B∆∞·ªõc 7: Sync l√™n Algolia**
```typescript
// storage/AlgoliaSync.ts
import algoliasearch from 'algoliasearch';

const client = algoliasearch('3JGCR12NR5', 'd8e34f818e6a139b73220857f9c3c5b7');
const jobsIndex = client.initIndex('jobs');

async function syncToAlgolia(jobs: any[]) {
  const algoliaObjects = jobs.map(job => ({
    objectID: job.id,
    title: job.title,
    company_name: job.company_name,
    location: job.location,
    job_type_id: job.job_type_id,
    category: job.category,
    salary_min: job.salary_min,
    salary_max: job.salary_max,
    skills: job.skills,
    description: stripHTML(job.description).substring(0, 1000), // Limit for indexing
    status: job.status,
    created_at: job.created_at,
  }));
  
  await jobsIndex.saveObjects(algoliaObjects);
  console.log(`Synced ${algoliaObjects.length} jobs to Algolia`);
}
```

---

## üîß X·ª¨ L√ù V·∫§N ƒê·ªÄ EMPLOYER & APPLICATION

### V·∫•n ƒë·ªÅ:
- Job crawl t·ª´ viecoi.vn kh√¥ng c√≥ employer th·∫≠t trong h·ªá th·ªëng Job_4S
- Candidate nh·∫•n "Apply" ‚Üí G·ª≠i CV cho ai?

### Gi·∫£i ph√°p:

#### **Job t·ª´ viecoi.vn (ngu·ªìn b√™n ngo√†i):**
```typescript
// Schema job
{
  source: "viecoi",
  external_url: "https://viecoi.vn/viec-lam/...",
  application_type: "external", // Redirect v·ªÅ ngu·ªìn
  employer_id: null,             // Kh√¥ng l∆∞u employer
  company_name: "FPT Software",  // Ch·ªâ l∆∞u t√™n, kh√¥ng t·∫°o account
}
```

**Flow ·ª©ng tuy·ªÉn:**
- Candidate xem job ‚Üí Nh·∫•n "Apply"
- App hi·ªÉn th·ªã modal x√°c nh·∫≠n:
  ```
  ‚ö†Ô∏è Tin tuy·ªÉn d·ª•ng t·ª´ Viecoi.vn
  
  Job n√†y ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ ngu·ªìn b√™n ngo√†i.
  B·∫°n s·∫Ω ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn trang g·ªëc ƒë·ªÉ ·ª©ng tuy·ªÉn tr·ª±c ti·∫øp v·ªõi nh√† tuy·ªÉn d·ª•ng.
  
  [Ti·∫øp t·ª•c ·ª©ng tuy·ªÉn] [H·ªßy]
  ```
- Nh·∫•n "Ti·∫øp t·ª•c" ‚Üí `Linking.openURL(job.external_url)` (m·ªü browser/WebView)
- Job_4S ghi log: `applications` collection v·ªõi status = "redirected"

---

#### **Job do employer trong h·ªá th·ªëng t·∫°o:**
```typescript
// Schema job
{
  source: "internal",
  external_url: null,
  application_type: "internal",
  employer_id: "real-employer-id", // Employer ƒë√£ ƒëƒÉng k√Ω
}
```

**Flow ·ª©ng tuy·ªÉn:**
- Candidate nh·∫•n Apply ‚Üí Upload CV trong app
- G·ª≠i v√†o collection `applications`
- Employer nh·∫≠n th√¥ng b√°o, xem CV, li√™n h·ªá candidate

---

#### **Disclaimer trong UI:**
```tsx
// JobDetailScreen.tsx
{job.source === 'viecoi' && (
  <View style={styles.disclaimer}>
    <Ionicons name="information-circle-outline" size={20} color="#f59e0b" />
    <Text style={styles.disclaimerText}>
      Tin t·ª´ Viecoi.vn. Khi ·ª©ng tuy·ªÉn, b·∫°n s·∫Ω ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn trang g·ªëc.
    </Text>
  </View>
)}

// Apply button
{job.application_type === 'external' ? (
  <TouchableOpacity onPress={() => handleExternalApply(job)}>
    <Text>·ª®ng tuy·ªÉn tr√™n Viecoi.vn ‚Üí</Text>
  </TouchableOpacity>
) : (
  <TouchableOpacity onPress={() => handleInternalApply(job)}>
    <Text>·ª®ng tuy·ªÉn ngay</Text>
  </TouchableOpacity>
)}
```

---

## üìä SCHEMA FIRESTORE (ƒêI·ªÄU CH·ªàNH)

### Collection: `jobs`
```typescript
{
  id: string;                  // Slug: "senior-fullstack-developer-fpt-software"
  title: string;
  company_id: string | null;   // Tham chi·∫øu companies (n·∫øu l√† internal)
  company_name: string;        // T√™n c√¥ng ty (cho external)
  company_logo?: string;       // URL logo (n·∫øu crawl ƒë∆∞·ª£c)
  
  location: string;            // B√¨nh D∆∞∆°ng, TP.HCM, H√† N·ªôi...
  job_type_id: string;         // full-time, part-time, intern, freelance
  category: string;            // C√¥ng ngh·ªá th√¥ng tin, Marketing, K·∫ø to√°n...
  
  salary_min?: number;
  salary_max?: number;
  currency: string;            // VND, USD
  salary_text?: string;        // "Th·ªèa thu·∫≠n", "L√™n ƒë·∫øn 30 tri·ªáu"
  
  skills: string[];
  description: string;         // HTML or plain text
  requirements: string[];
  benefits?: string[];
  
  experience?: string;         // "1-2 nƒÉm", "Kh√¥ng y√™u c·∫ßu"
  education?: string;          // "ƒê·∫°i h·ªçc", "Cao ƒë·∫≥ng"
  quantity?: number;           // S·ªë l∆∞·ª£ng tuy·ªÉn
  gender?: string;             // "Nam/N·ªØ", "Kh√¥ng y√™u c·∫ßu"
  
  // Metadata
  source: "viecoi" | "internal" | "manual";
  application_type: "internal" | "external";
  external_url?: string;       // URL g·ªëc n·∫øu t·ª´ ngu·ªìn ngo√†i
  is_verified: boolean;        // Admin ƒë√£ ki·ªÉm duy·ªát
  is_crawled: boolean;         // True n·∫øu t·ª´ crawl
  
  // Timestamps
  created_at: Timestamp;
  updated_at: Timestamp;
  expires_at?: Timestamp;      // Deadline
  crawled_at?: Timestamp;      // L·∫ßn crawl cu·ªëi
  
  // Status
  status: "draft" | "active" | "closed" | "expired";
  
  // Stats (optional)
  view_count?: number;
  application_count?: number;
  save_count?: number;
}
```

### Collection: `companies`
```typescript
{
  id: string;                  // Slug: "fpt-software"
  name: string;
  logo?: string;
  industry: string;            // C√¥ng ngh·ªá th√¥ng tin, T√†i ch√≠nh...
  address: string;
  website?: string;
  size?: string;               // "50-100", "100-500", "500+"
  description?: string;
  
  // Metadata
  source: "viecoi" | "internal";
  external_url?: string;
  is_verified: boolean;
  
  // Timestamps
  created_at: Timestamp;
  updated_at: Timestamp;
  
  // Stats
  job_count?: number;          // S·ªë job ƒëang tuy·ªÉn
}
```

### Collection: `crawl_logs`
```typescript
{
  id: string;
  source: "viecoi";
  type: "sitemap" | "jobs" | "companies";
  started_at: Timestamp;
  completed_at: Timestamp;
  status: "success" | "failed" | "partial";
  
  stats: {
    total_urls: number;
    success_count: number;
    failed_count: number;
    new_jobs: number;
    updated_jobs: number;
    new_companies: number;
  };
  
  errors?: Array<{
    url: string;
    error: string;
  }>;
}
```

---

## üé® ADMIN UI

### M√†n h√¨nh: `CrawledJobsManagement.tsx`
```tsx
function CrawledJobsManagement() {
  const [jobs, setJobs] = useState([]);
  const [filter, setFilter] = useState({ 
    source: 'viecoi',  // viecoi, internal, all
    status: 'draft'     // draft, active, closed
  });
  const [stats, setStats] = useState({
    total: 0,
    pending: 0,
    approved: 0,
    rejected: 0
  });

  return (
    <ScrollView>
      {/* Stats Cards */}
      <StatsCards stats={stats} />
      
      {/* Filters */}
      <View style={styles.filters}>
        <Picker value={filter.source} onChange={(v) => setFilter({...filter, source: v})}>
          <Picker.Item label="T·∫•t c·∫£" value="all" />
          <Picker.Item label="Viecoi.vn" value="viecoi" />
          <Picker.Item label="N·ªôi b·ªô" value="internal" />
        </Picker>
        
        <Picker value={filter.status} onChange={(v) => setFilter({...filter, status: v})}>
          <Picker.Item label="Ch·ªù duy·ªát" value="draft" />
          <Picker.Item label="ƒê√£ duy·ªát" value="active" />
          <Picker.Item label="ƒê√£ ƒë√≥ng" value="closed" />
        </Picker>
      </View>
      
      {/* Jobs Table */}
      <FlatList
        data={jobs}
        keyExtractor={(item) => item.id}
        renderItem={({ item }) => (
          <JobRow
            job={item}
            onApprove={() => approveJob(item.id)}
            onReject={() => rejectJob(item.id)}
            onEdit={() => navigateToEdit(item.id)}
            onViewOriginal={() => Linking.openURL(item.external_url)}
          />
        )}
      />
    </ScrollView>
  );
}

function JobRow({ job, onApprove, onReject, onEdit, onViewOriginal }) {
  return (
    <View style={styles.jobRow}>
      <View style={styles.jobInfo}>
        <Text style={styles.jobTitle}>{job.title}</Text>
        <Text style={styles.jobCompany}>{job.company_name}</Text>
        <Text style={styles.jobLocation}>{job.location} ‚Ä¢ {job.salary_text}</Text>
        <View style={styles.badges}>
          <Badge color="blue">{job.source}</Badge>
          <Badge color={job.status === 'draft' ? 'orange' : 'green'}>{job.status}</Badge>
        </View>
      </View>
      
      <View style={styles.actions}>
        {job.status === 'draft' && (
          <>
            <Button title="‚úì Duy·ªát" onPress={onApprove} color="green" />
            <Button title="‚úï T·ª´ ch·ªëi" onPress={onReject} color="red" />
          </>
        )}
        <Button title="‚úé S·ª≠a" onPress={onEdit} />
        {job.external_url && (
          <Button title="‚Üó Xem g·ªëc" onPress={onViewOriginal} />
        )}
      </View>
    </View>
  );
}
```

**Actions:**
- **Approve**: Set `status: "active"`, `is_verified: true` ‚Üí Hi·ªÉn th·ªã cho candidate
- **Reject**: Set `status: "closed"` ho·∫∑c delete
- **Edit**: Navigate to form edit (s·ª≠a JD, l∆∞∆°ng, k·ªπ nƒÉng...)
- **View Original**: M·ªü link g·ªëc tr√™n viecoi.vn ƒë·ªÉ ki·ªÉm tra

---

### M√†n h√¨nh: `CrawlLogsScreen.tsx`
```tsx
function CrawlLogsScreen() {
  const [logs, setLogs] = useState([]);
  
  return (
    <ScrollView>
      <Text style={styles.title}>L·ªãch s·ª≠ crawl</Text>
      
      <FlatList
        data={logs}
        keyExtractor={(item) => item.id}
        renderItem={({ item }) => (
          <View style={styles.logRow}>
            <Text style={styles.logDate}>
              {item.completed_at.toDate().toLocaleString('vi-VN')}
            </Text>
            <Text style={styles.logType}>{item.type} t·ª´ {item.source}</Text>
            <Text style={styles.logStats}>
              ‚úì {item.stats.success_count} / {item.stats.total_urls} URLs
              ‚Ä¢ {item.stats.new_jobs} jobs m·ªõi
              ‚Ä¢ {item.stats.updated_jobs} jobs c·∫≠p nh·∫≠t
            </Text>
            {item.stats.failed_count > 0 && (
              <Text style={styles.logErrors}>
                ‚ö† {item.stats.failed_count} l·ªói
              </Text>
            )}
          </View>
        )}
      />
    </ScrollView>
  );
}
```

---

## üöÄ DEPLOYMENT & CH·∫†Y TH·ª¨

### **Setup l·∫ßn ƒë·∫ßu:**
```bash
# 1. Clone repo
cd JobApplication/server

# 2. Install dependencies
npm install axios cheerio xml2js fuse.js winston

# 3. Setup .env
cp .env.example .env
# Edit .env v·ªõi credentials:
FIREBASE_SERVICE_ACCOUNT_PATH=./serviceAccountKey.json
ALGOLIA_APP_ID=3JGCR12NR5
ALGOLIA_API_KEY=d8e34f818e6a139b73220857f9c3c5b7

# 4. Test crawl sitemap (l·∫•y 10 jobs ƒë·∫ßu ti√™n)
npm run crawl:viecoi-test

# 5. Crawl full (100 jobs)
npm run crawl:viecoi-jobs

# 6. Sync l√™n Algolia
npm run sync:to-algolia

# 7. Check Firestore Console
# https://console.firebase.google.com/project/YOUR_PROJECT/firestore
```

### **Package.json scripts:**
```json
{
  "scripts": {
    "crawl:viecoi-test": "ts-node src/crawlers/viecoi/index.ts --limit 10",
    "crawl:viecoi-jobs": "ts-node src/crawlers/viecoi/index.ts --type jobs --limit 100",
    "crawl:viecoi-companies": "ts-node src/crawlers/viecoi/index.ts --type companies --limit 50",
    "crawl:viecoi-full": "npm run crawl:viecoi-jobs && npm run crawl:viecoi-companies",
    "sync:to-algolia": "ts-node src/scripts/sync-to-algolia.ts",
    "crawl:scheduled": "npm run crawl:viecoi-full && npm run sync:to-algolia"
  }
}
```

---

### **GitHub Actions setup:**
```bash
# 1. V√†o GitHub repo ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions
# 2. Th√™m secrets:

Name: FIREBASE_SERVICE_ACCOUNT
Value: <paste to√†n b·ªô n·ªôi dung serviceAccountKey.json>

Name: ALGOLIA_APP_ID
Value: 3JGCR12NR5

Name: ALGOLIA_API_KEY
Value: d8e34f818e6a139b73220857f9c3c5b7

# 3. T·∫°o file .github/workflows/sync-jobs-viecoi.yml (ƒë√£ c√≥ ·ªü tr√™n)

# 4. Push code l√™n GitHub
git add .
git commit -m "Add viecoi crawler with GitHub Actions"
git push origin main

# 5. Enable Actions:
# V√†o GitHub repo ‚Üí Actions ‚Üí Enable workflows

# 6. Test run th·ªß c√¥ng:
# V√†o Actions tab ‚Üí Select "Sync Jobs from ViecoiVN" ‚Üí Run workflow

# 7. Check logs ƒë·ªÉ debug n·∫øu c√≥ l·ªói
```

---

### **Ch·∫°y th·ªß c√¥ng (local):**
```bash
# Terminal 1: Crawl jobs
npm run crawl:viecoi-jobs

# Terminal 2: Monitor logs
tail -f logs/crawler.log

# Sau khi crawl xong, check Firestore:
# 1. M·ªü Firebase Console
# 2. V√†o Firestore ‚Üí jobs collection
# 3. Verify: source = "viecoi", status = "draft"

# Sync l√™n Algolia:
npm run sync:to-algolia

# Check Algolia Dashboard:
# https://www.algolia.com/apps/3JGCR12NR5/explorer/browse/jobs
```

---

### **Rate limiting & Best practices:**
```typescript
// utils/rateLimiter.ts
export class RateLimiter {
  private queue: Array<() => Promise<void>> = [];
  private running = 0;
  
  constructor(
    private maxRequests: number = 10,  // 10 requests
    private perSeconds: number = 1     // per second
  ) {}
  
  async wait() {
    if (this.running >= this.maxRequests) {
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    this.running++;
    setTimeout(() => this.running--, this.perSeconds * 1000);
  }
}

// Usage:
const limiter = new RateLimiter({ maxRequests: 5, perSeconds: 1 }); // 5 req/s

for (const url of urls) {
  await limiter.wait();
  await crawlJob(url);
}
```

**Best practices:**
- Crawl v√†o gi·ªù th·∫•p ƒëi·ªÉm (2-4 AM) ƒë·ªÉ gi·∫£m t·∫£i server
- D√πng User-Agent r√µ r√†ng: `Job4S-Crawler/1.0 (Educational Project)`
- Retry khi g·∫∑p l·ªói 5xx (server error)
- Cache sitemap ƒë·ªÉ tr√°nh re-crawl to√†n b·ªô
- Log chi ti·∫øt ƒë·ªÉ d·ªÖ debug

---

## üìà ROADMAP TH·ª∞C T·∫æ (CHO ƒê·ªí √ÅN)

### **Tu·∫ßn 1: Setup crawler c∆° b·∫£n**
- [x] Ph√¢n t√≠ch robots.txt viecoi.vn ‚Üí X√°c nh·∫≠n h·ª£p ph√°p
- [ ] Setup project structure (`/server/src/crawlers/viecoi/`)
- [ ] Install dependencies (axios, cheerio, xml2js, fuse.js, winston)
- [ ] Vi·∫øt `SitemapCrawler.ts` ‚Üí Crawl sitemap.xml
- [ ] Test v·ªõi 10 URLs ƒë·∫ßu ti√™n
- [ ] L∆∞u raw data v√†o file JSON

**M·ª•c ti√™u**: L·∫•y ƒë∆∞·ª£c danh s√°ch URL jobs v√† companies t·ª´ sitemap

---

### **Tu·∫ßn 2: Crawl job details**
- [ ] Inspect HTML structure c·ªßa viecoi.vn (DevTools)
- [ ] Vi·∫øt `JobParser.ts` ‚Üí Extract JD, l∆∞∆°ng, k·ªπ nƒÉng, c√¥ng ty...
- [ ] Implement rate limiting (5-10 req/s)
- [ ] Vi·∫øt `JobCrawler.ts` ‚Üí Crawl 50-100 jobs
- [ ] Test data quality, adjust selectors n·∫øu c·∫ßn
- [ ] L∆∞u v√†o `viecoi-jobs-raw.json`

**M·ª•c ti√™u**: Crawl ƒë∆∞·ª£c 50-100 jobs v·ªõi JD ƒë·∫ßy ƒë·ªß

---

### **Tu·∫ßn 3: Normalize & Deduplicate**
- [ ] Vi·∫øt `Normalizer.ts` ‚Üí Map v·ªÅ schema Job_4S
- [ ] Map location, job_type, category v·ªÅ taxonomy
- [ ] Parse salary text ‚Üí salary_min, salary_max
- [ ] Extract skills t·ª´ JD
- [ ] Vi·∫øt `Deduplicator.ts` ‚Üí Fuzzy match ƒë·ªÉ tr√°nh tr√πng
- [ ] Test v·ªõi sample data

**M·ª•c ti√™u**: D·ªØ li·ªáu s·∫°ch, chu·∫©n h√≥a, kh√¥ng tr√πng l·∫∑p

---

### **Tu·∫ßn 4: Upsert Firestore & Algolia**
- [ ] Vi·∫øt `FirestoreUpserter.ts` ‚Üí Batch upsert jobs
- [ ] Test upsert 10 jobs ‚Üí Verify tr√™n Firestore Console
- [ ] Vi·∫øt `AlgoliaSync.ts` ‚Üí Sync jobs l√™n Algolia
- [ ] Test search tr√™n Algolia Dashboard
- [ ] Handle errors, retry logic

**M·ª•c ti√™u**: Jobs hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß tr√™n app, search ho·∫°t ƒë·ªông

---

### **Tu·∫ßn 5: Crawl companies & Admin UI**
- [ ] Vi·∫øt `CompanyCrawler.ts` + `CompanyParser.ts`
- [ ] Crawl 30-50 companies
- [ ] Link companies v·ªõi jobs (match by name)
- [ ] X√¢y d·ª±ng `CrawledJobsManagement` screen
- [ ] Implement approve/reject actions
- [ ] Test flow ki·ªÉm duy·ªát

**M·ª•c ti√™u**: Admin c√≥ th·ªÉ ki·ªÉm duy·ªát jobs, companies

---

### **Tu·∫ßn 6: GitHub Actions & Scheduling**
- [ ] T·∫°o `.github/workflows/sync-jobs-viecoi.yml`
- [ ] Setup secrets tr√™n GitHub
- [ ] Test workflow ch·∫°y th·ªß c√¥ng
- [ ] Enable schedule (m·ªói ng√†y 02:00)
- [ ] Monitor logs, fix bugs n·∫øu c√≥

**M·ª•c ti√™u**: Crawler ch·∫°y t·ª± ƒë·ªông h√†ng ng√†y

---

### **Tu·∫ßn 7: Polish & Documentation**
- [ ] Implement disclaimer UI cho external jobs
- [ ] Test flow ·ª©ng tuy·ªÉn (redirect v·ªÅ viecoi.vn)
- [ ] Vi·∫øt `CrawlLogsScreen` ƒë·ªÉ theo d√µi l·ªãch s·ª≠
- [ ] Optimize performance (caching, pagination...)
- [ ] Vi·∫øt README.md h∆∞·ªõng d·∫´n setup & ch·∫°y crawler
- [ ] Chu·∫©n b·ªã demo cho h·ªôi ƒë·ªìng

**M·ª•c ti√™u**: H·ªá th·ªëng ho√†n ch·ªânh, s·∫µn s√†ng demo

---

### **Tu·∫ßn 8+: M·ªü r·ªông (optional)**
- [ ] Crawl th√™m categories (marketing, k·∫ø to√°n, b√°n h√†ng...)
- [ ] Implement incremental crawl (ch·ªâ crawl jobs m·ªõi/update)
- [ ] Add analytics: job view count, application rate...
- [ ] AI suggestion: g·ª£i √Ω job ph√π h·ª£p v·ªõi candidate
- [ ] Enrich data: salary prediction, skill extraction...

**M·ª•c ti√™u**: N√¢ng cao ch·∫•t l∆∞·ª£ng, tƒÉng gi√° tr·ªã ƒë·ªì √°n

---

## ‚öñÔ∏è PH√ÅP L√ù & DISCLAIMER

### **Tu√¢n th·ªß robots.txt viecoi.vn:**
- ‚úÖ Ch·ªâ crawl c√°c URL ƒë∆∞·ª£c `Allow`:
  - `/viec-lam/*.html` (job details)
  - `/tim-cong-ty/*.html`, `/gioi-thieu-cong-ty/*.html` (company details)
  - `sitemap.xml`
- ‚ùå Kh√¥ng crawl c√°c URL b·ªã `Disallow`:
  - `/admin/*`, `/employer/*`, `/jobseeker/*`
  - C√°c trang test, demo

### **T√¥n tr·ªçng b·∫£n quy·ªÅn:**
- L∆∞u to√†n b·ªô JD, m√¥ t·∫£ c√¥ng ty v√¨ ƒë∆∞·ª£c ph√©p crawl
- Ghi r√µ ngu·ªìn: `source: "viecoi"`, `external_url: "https://viecoi.vn/..."`
- Disclaimer r√µ r√†ng trong app: "Tin t·ª´ Viecoi.vn"

### **Kh√¥ng g√¢y qu√° t·∫£i server:**
- Rate limiting: 5-10 requests/gi√¢y
- Crawl v√†o gi·ªù th·∫•p ƒëi·ªÉm (2-4 AM)
- User-Agent r√µ r√†ng: `Job4S-Crawler/1.0 (Educational Project)`
- Retry v·ªõi exponential backoff khi g·∫∑p l·ªói

### **Trong app, th√™m disclaimer:**
```tsx
// Settings/About screen
<View style={styles.disclaimer}>
  <Text style={styles.disclaimerTitle}>Ngu·ªìn d·ªØ li·ªáu</Text>
  <Text style={styles.disclaimerText}>
    Job4S t·ªïng h·ª£p th√¥ng tin tuy·ªÉn d·ª•ng t·ª´ Viecoi.vn v√† c√°c ngu·ªìn c√¥ng khai kh√°c.
    Khi ·ª©ng tuy·ªÉn, b·∫°n s·∫Ω ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn trang g·ªëc ƒë·ªÉ li√™n h·ªá tr·ª±c ti·∫øp v·ªõi nh√† tuy·ªÉn d·ª•ng.
    Ch√∫ng t√¥i kh√¥ng ch·ªãu tr√°ch nhi·ªám v·ªÅ n·ªôi dung tin tuy·ªÉn d·ª•ng t·ª´ ngu·ªìn b√™n ngo√†i.
  </Text>
</View>

// JobDetailScreen cho external jobs
{job.source === 'viecoi' && (
  <View style={styles.sourceNote}>
    <Text>üìå Tin t·ª´ Viecoi.vn</Text>
    <Text style={styles.noteText}>
      ·ª®ng tuy·ªÉn s·∫Ω chuy·ªÉn ƒë·∫øn trang g·ªëc. Job4S kh√¥ng tr·ª±c ti·∫øp x·ª≠ l√Ω ·ª©ng tuy·ªÉn n√†y.
    </Text>
  </View>
)}
```

### **Trong b√°o c√°o ƒë·ªì √°n, ghi r√µ:**
```
PH·∫¶N: THU TH·∫¨P D·ªÆ LI·ªÜU

1. Ngu·ªìn d·ªØ li·ªáu: Viecoi.vn
2. Ph∆∞∆°ng ph√°p: Web crawling h·ª£p ph√°p (tu√¢n th·ªß robots.txt)
3. Quy tr√¨nh:
   - Crawl sitemap.xml ƒë·ªÉ l·∫•y danh s√°ch URL jobs v√† companies
   - Crawl t·ª´ng trang chi ti·∫øt ƒë·ªÉ extract th√¥ng tin
   - Normalize v√† deduplicate d·ªØ li·ªáu
   - Upsert v√†o Firestore, sync l√™n Algolia
4. Tu√¢n th·ªß ph√°p l√Ω:
   - Ch·ªâ crawl c√°c URL ƒë∆∞·ª£c ph√©p trong robots.txt
   - Ghi r√µ ngu·ªìn, kh√¥ng x√≥a link g·ªëc
   - Rate limiting ƒë·ªÉ kh√¥ng g√¢y qu√° t·∫£i
   - Disclaimer r√µ r√†ng trong app
5. L√Ω do ch·ªçn viecoi.vn:
   - TopCV, VietnamWorks ch·∫∑n crawl trong robots.txt
   - Viecoi.vn cho ph√©p crawl job v√† company details
   - D·ªØ li·ªáu Vi·ªát Nam, ph√π h·ª£p v·ªõi ƒë·ªëi t∆∞·ª£ng sinh vi√™n
```

---

## üéØ K·∫æT LU·∫¨N

### **ƒêi·ªÅu ch·ªânh quan tr·ªçng t·ª´ plan c≈©:**
1. ‚ùå **TopCV, VietnamWorks ch·∫∑n crawl** ‚Üí ‚úÖ **Chuy·ªÉn sang viecoi.vn (cho ph√©p crawl)**
2. ‚ùå **Ch·ªâ c√≥ metadata c∆° b·∫£n** ‚Üí ‚úÖ **Crawl ƒë∆∞·ª£c JD ƒë·∫ßy ƒë·ªß t·ª´ viecoi.vn**
3. ‚ùå **Dataset th·ªß c√¥ng t·ªën th·ªùi gian** ‚Üí ‚úÖ **Crawler t·ª± ƒë·ªông, c√≥ th·ªÉ crawl h√†ng trƒÉm jobs**
4. ‚úÖ **GitHub Actions** thay v√¨ cron 24/7 (gi·ªØ nguy√™n)
5. ‚úÖ **Redirect external jobs** thay v√¨ l∆∞u employer gi·∫£ (gi·ªØ nguy√™n)
6. ‚úÖ **Disclaimer r√µ r√†ng** trong UI (gi·ªØ nguy√™n)

### **∆Øu ƒëi·ªÉm c·ªßa gi·∫£i ph√°p m·ªõi:**
- **H·ª£p ph√°p 100%**: Viecoi.vn cho ph√©p crawl trong robots.txt
- **D·ªØ li·ªáu Vi·ªát Nam**: Ph√π h·ª£p v·ªõi sinh vi√™n t·∫°i B√¨nh D∆∞∆°ng v√† c√°c t·ªânh l√¢n c·∫≠n
- **JD ƒë·∫ßy ƒë·ªß**: C√≥ ƒë·ªß th√¥ng tin ƒë·ªÉ demo ch·ª©c nƒÉng t√¨m ki·∫øm, ·ª©ng tuy·ªÉn
- **T·ª± ƒë·ªông h√≥a**: Crawler ch·∫°y h√†ng ng√†y, c·∫≠p nh·∫≠t jobs m·ªõi
- **Kh√¥ng t·ªën ti·ªÅn**: GitHub Actions mi·ªÖn ph√≠, kh√¥ng c·∫ßn server ri√™ng
- **D·ªÖ maintain**: Code structure r√µ r√†ng, d·ªÖ debug v√† m·ªü r·ªông

### **Ph√π h·ª£p v·ªõi y√™u c·∫ßu ƒë·ªì √°n:**
| Y√™u c·∫ßu ƒë·ªÅ t√†i | Gi·∫£i ph√°p |
|---------------|-----------|
| Thu th·∫≠p d·ªØ li·ªáu tuy·ªÉn d·ª•ng 24/7 | ‚úÖ GitHub Actions ch·∫°y h√†ng ng√†y |
| AI & Web Scraping | ‚úÖ Cheerio + Puppeteer (n·∫øu c·∫ßn) |
| T√≠ch h·ª£p search engine | ‚úÖ Algolia ƒë√£ setup |
| K·∫øt n·ªëi sinh vi√™n - nh√† tuy·ªÉn d·ª•ng | ‚úÖ Redirect v·ªÅ viecoi.vn ho·∫∑c apply n·ªôi b·ªô |
| G·ª£i √Ω c√¥ng vi·ªác th√¥ng minh | ‚úÖ C√≥ ƒë·ªß data (k·ªπ nƒÉng, ƒë·ªãa ƒëi·ªÉm, l∆∞∆°ng) |
| Qu·∫£n l√Ω CV, ·ª©ng tuy·ªÉn | ‚úÖ Internal jobs h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß |

### **∆Ø·ªõc t√≠nh th·ªùi gian:**
- **Tu·∫ßn 1-3**: Crawler c∆° b·∫£n + crawl 100 jobs (c·ªët l√µi)
- **Tu·∫ßn 4-5**: Normalize, upsert Firestore, sync Algolia
- **Tu·∫ßn 6-7**: Admin UI, GitHub Actions, disclaimer
- **Tu·∫ßn 8+**: Polish, optimize, chu·∫©n b·ªã demo

**T·ªïng th·ªùi gian**: 6-8 tu·∫ßn (th·ª±c t·∫ø cho sinh vi√™n)

### **Roadmap chi ti·∫øt:**
```
PHASE 1 (Tu·∫ßn 1-3): CORE CRAWLER ‚≠ê ∆Øu ti√™n cao
‚îú‚îÄ Setup project structure
‚îú‚îÄ Crawl sitemap viecoi.vn
‚îú‚îÄ Crawl 50-100 job details
‚îî‚îÄ Test data quality

PHASE 2 (Tu·∫ßn 4-5): DATA PROCESSING ‚≠ê ∆Øu ti√™n cao
‚îú‚îÄ Normalize data v·ªÅ schema Job_4S
‚îú‚îÄ Deduplicate jobs
‚îú‚îÄ Upsert v√†o Firestore
‚îî‚îÄ Sync l√™n Algolia

PHASE 3 (Tu·∫ßn 6-7): AUTOMATION & UI ‚≠ê ∆Øu ti√™n trung b√¨nh
‚îú‚îÄ GitHub Actions workflow
‚îú‚îÄ Admin UI (approve/reject)
‚îú‚îÄ Disclaimer cho external jobs
‚îî‚îÄ Test end-to-end flow

PHASE 4 (Tu·∫ßn 8+): POLISH & EXTEND üé® Optional
‚îú‚îÄ Crawl companies
‚îú‚îÄ Crawl logs screen
‚îú‚îÄ Incremental crawl
‚îî‚îÄ Analytics dashboard
```

---

## üìù CHECKLIST TR∆Ø·ªöC KHI B·∫ÆT ƒê·∫¶U

### **K·ªπ thu·∫≠t:**
- [ ] ƒê√£ c√≥ Firebase project + serviceAccountKey.json
- [ ] ƒê√£ c√≥ Algolia account + API keys
- [ ] ƒê√£ c√†i Node.js v18+, npm, TypeScript
- [ ] ƒê√£ test axios, cheerio tr√™n terminal

### **Ph√°p l√Ω:**
- [ ] ƒê√£ ƒë·ªçc k·ªπ robots.txt c·ªßa viecoi.vn
- [ ] Hi·ªÉu r√µ nh·ªØng URL n√†o ƒë∆∞·ª£c ph√©p crawl
- [ ] Chu·∫©n b·ªã disclaimer cho app v√† b√°o c√°o
- [ ] S·∫µn s√†ng gi·∫£i th√≠ch v·ªõi h·ªôi ƒë·ªìng n·∫øu b·ªã h·ªèi

### **L·∫≠p k·∫ø ho·∫°ch:**
- [ ] ƒê√£ ƒë·ªçc to√†n b·ªô plan n√†y
- [ ] Hi·ªÉu r√µ quy tr√¨nh crawl 7 b∆∞·ªõc
- [ ] X√°c ƒë·ªãnh timeline ph√π h·ª£p (6-8 tu·∫ßn)
- [ ] Chu·∫©n b·ªã backup plan n·∫øu viecoi.vn thay ƒë·ªïi HTML

---

## üöÄ B∆Ø·ªöC TI·∫æP THEO NGAY B√ÇY GI·ªú

1. **Inspect viecoi.vn** (5 ph√∫t):
   - V√†o https://viecoi.vn/viec-lam/
   - M·ªü DevTools (F12) ‚Üí Elements
   - T√¨m c√°c class/id cho: title, company, salary, location, description...
   - Ghi l·∫°i ƒë·ªÉ vi·∫øt parser

2. **Test crawl sitemap** (10 ph√∫t):
   ```bash
   cd JobApplication/server
   npm install axios xml2js
   node -e "
   const axios = require('axios');
   const xml2js = require('xml2js');
   (async () => {
     const res = await axios.get('https://viecoi.vn/sitemap.xml');
     const parsed = await xml2js.parseStringPromise(res.data);
     console.log('Total URLs:', parsed.urlset.url.length);
     console.log('Sample URL:', parsed.urlset.url[0].loc[0]);
   })();
   "
   ```

3. **Test crawl 1 job** (15 ph√∫t):
   ```bash
   npm install cheerio
   node -e "
   const axios = require('axios');
   const cheerio = require('cheerio');
   (async () => {
     const url = 'https://viecoi.vn/viec-lam/PASTE_URL_HERE.html';
     const res = await axios.get(url);
     const $ = cheerio.load(res.data);
     console.log('Title:', $('.job-title').text().trim());
     console.log('Company:', $('.company-name').text().trim());
   })();
   "
   ```

4. **T·∫°o project structure** (10 ph√∫t):
   ```bash
   mkdir -p server/src/crawlers/viecoi/{parsers,utils}
   mkdir -p server/src/crawlers/processors
   mkdir -p server/src/crawlers/storage
   mkdir -p server/data
   mkdir -p .github/workflows
   ```

5. **B·∫Øt ƒë·∫ßu code** ‚Üí Theo roadmap tu·∫ßn 1!

---

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi ƒë·ªì √°n! üéì**

**Ngu·ªìn h·ª£p ph√°p + Crawler t·ª± ƒë·ªông + GitHub Actions = Gi·∫£i ph√°p ho√†n h·∫£o cho sinh vi√™n!**
